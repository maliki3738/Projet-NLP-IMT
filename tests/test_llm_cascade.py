#!/usr/bin/env python3
"""Test cascade LLM et tracking coÃ»ts Langfuse"""

print("ğŸ§ª Test nouvelle cascade LLM (Gemini â†’ Grok â†’ OpenAI)")
print("=" * 60)

# Import avec gestion des erreurs
try:
    from app.agent import _call_gemini
    print("âœ… Import _call_gemini OK")
except Exception as e:
    print(f"âŒ Erreur import: {e}")
    exit(1)

# Afficher la docstring
print("\nğŸ“– Documentation:")
print(_call_gemini.__doc__)

# Afficher l'ordre de prioritÃ©
print("\nğŸ¯ Ordre d'appel configurÃ©:")
print("   1. ğŸ¥‡ Gemini (gemini-2.0-flash-exp) - GRATUIT")
print("      â€¢ Free tier: 15 req/min, 1500 req/jour")
print("      â€¢ CoÃ»t: 0$ (tracking tokens uniquement)")
print("")
print("   2. ğŸ¥ˆ Grok (grok-beta)")
print("      â€¢ CoÃ»t: 5$/1M input + 15$/1M output")
print("      â€¢ Tracking: tokens + coÃ»ts USD dans Langfuse")
print("")
print("   3. ğŸ¥‰ OpenAI (gpt-4o-mini)")
print("      â€¢ CoÃ»t: 0.15$/1M input + 0.60$/1M output")
print("      â€¢ Tracking: tokens + coÃ»ts USD dans Langfuse")

print("\nğŸ’¡ Tous les appels sont trackÃ©s dans Langfuse avec:")
print("   â€¢ Prompt envoyÃ©")
print("   â€¢ RÃ©ponse reÃ§ue")
print("   â€¢ Usage tokens (prompt + completion)")
print("   â€¢ CoÃ»t estimÃ© en USD")
print("   â€¢ MÃ©tadonnÃ©es (modÃ¨le, tempÃ©rature, max_tokens)")

print("\nğŸ“Š Dashboard Langfuse: https://cloud.langfuse.com")
print("   Onglet 'Traces' pour voir tous les appels")

print("\nâœ… Configuration terminÃ©e !")
