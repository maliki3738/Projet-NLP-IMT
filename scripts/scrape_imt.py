#scripts/scrape_imt.py
import requests
from bs4 import BeautifulSoup
from pathlib import Path
import re

BASE_URL = "https://www.imt.sn"
PAGES = {
    "accueil": BASE_URL,
    "formations": f"{BASE_URL}/bachelor-sciences-et-ingenierie-du-numerique-iot-cyber-cloud/",
    "formations_generale": f"{BASE_URL}/2-bachelors-en-sciences-et-ingenierie/",
    "institut_mines_telecom": f"{BASE_URL}/institut-mines-telecom/",
    "qui_sommes_nous": f"{BASE_URL}/qui-sommes-nous/institut-mines-telecom-dakar/",
    "Edulab": f"{BASE_URL}/espace-edulab/",
    "contact": f"{BASE_URL}/contact/",
}

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

def scrape_page(name, url):
    """Scraping optimis√© - extrait contenu informatif + donn√©es structur√©es (adresse, email, tel)."""
    print(f"üöÄ Scraping {name}...")
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # 1. Nettoyage : supprimer les √©l√©ments parasites
        for tag in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            tag.decompose()

        # 2. Extraction de donn√©es structur√©es (emails, t√©l√©phones, adresses)
        structured_data = []
        
        # Extraire emails
        emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', str(soup))
        if emails:
            structured_data.append(f"üìß Contact : {', '.join(set(emails))}")
        
        # Extraire t√©l√©phones (format international et local)
        phones = re.findall(r'(?:\+221|00221)?\s*\d{2}[\s.-]?\d{3}[\s.-]?\d{2}[\s.-]?\d{2}', str(soup))
        if phones:
            structured_data.append(f"üìû T√©l√©phone : {', '.join(set(phones))}")
        
        # Extraire adresses (recherche de patterns communs au S√©n√©gal)
        address_patterns = [
            r'(?i)(rue|avenue|boulevard|route|quartier|zone|immeuble)[^<>]{5,100}(?:dakar|s√©n√©gal|senegal)',
            r'(?i)(?:dakar|s√©n√©gal|senegal)[^<>]{5,100}(?:rue|avenue|boulevard|quartier)',
        ]
        for pattern in address_patterns:
            addresses = re.findall(pattern, str(soup))
            if addresses:
                structured_data.append(f"üìç Adresse : {addresses[0]}")
                break
        
        # 3. Extraction de contenu textuel
        text_blocks = []
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'address', 'span']):
            text = tag.get_text().strip()
            # Garder le texte significatif (> 10 chars pour capturer plus d'infos)
            if len(text) > 10:
                text_blocks.append(text)

        # 4. Filtrage du bruit critique (blacklist am√©lior√©e)
        BLACKLIST = [
            "accepter les cookies", "refuser les cookies", "politique de confidentialit√©",
            "google analytics", "google recaptcha", "combien font", "captcha",
            "pistage dans votre navigateur", "r√©glages des polices google", 
            "int√©grations de vid√©o", "page mentions l√©gales", "cookies et param√®tres",
            "nous utilisons des cookies", "bloquer les cookies", "effacer les cookies",
            "services externes", "google webfonts", "google maps", "h√©bergeurs de vid√©o",
            "adresse ip", "fai sont susceptibles", "rechargement de la page"
        ]
        
        def is_noise(line: str) -> bool:
            l = line.lower()
            return any(word in l for word in BLACKLIST) or len(line) < 15
        
        cleaned_blocks = [block for block in text_blocks if not is_noise(block)]
        
        # 5. D√©doublonnage (garder uniquement les blocs uniques)
        unique_blocks = []
        seen = set()
        for block in cleaned_blocks:
            normalized = re.sub(r'\s+', ' ', block.lower())  # Normaliser les espaces
            if normalized not in seen:
                seen.add(normalized)
                unique_blocks.append(block)

        # 6. Combiner donn√©es structur√©es + contenu
        final_content = structured_data + unique_blocks

        # 7. Sauvegarde
        if final_content:
            file_path = DATA_DIR / f"{name}.txt"
            file_path.write_text("\n\n".join(final_content), encoding="utf-8")
            print(f"‚úÖ {name}.txt sauvegard√© ({len(final_content)} blocs, dont {len(structured_data)} donn√©es structur√©es)")
        else:
            print(f"‚ö†Ô∏è Aucun contenu trouv√© pour {name}")

    except Exception as e:
        print(f"‚ùå Erreur sur {name}: {e}")


if __name__ == "__main__":
    print("=" * 60)
    print("üîç SCRAPING IMT DAKAR - Version Optimis√©e")
    print("=" * 60)
    
    for name, url in PAGES.items():
        scrape_page(name, url)
    
    print("\n" + "=" * 60)
    print("‚úÖ Scraping termin√© ! Relancez build_index.py pour reconstruire l'index.")
    print("=" * 60)