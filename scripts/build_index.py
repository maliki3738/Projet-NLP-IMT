# scripts/build_index.py
"""
Script pour crÃ©er l'index JSON des chunks Ã  partir des fichiers .txt
Ce script est obsolÃ¨te - utilisez build_vector_index.py Ã  la place pour FAISS
"""
from pathlib import Path
import json
import re

DATA_DIR = Path("data")
INDEX_FILE = DATA_DIR / "chunks.json"

def clean_text(text):
    """Nettoie le texte en supprimant les espaces multiples et lignes vides."""
    text = re.sub(r'\n\s*\n+', '\n\n', text)  # RÃ©duire lignes vides multiples
    text = re.sub(r' +', ' ', text)  # RÃ©duire espaces multiples
    return text.strip()

def split_into_paragraphs(text):
    """DÃ©coupe le texte en paragraphes cohÃ©rents (minimum 30 caractÃ¨res)."""
    # SÃ©parer par double saut de ligne (paragraphes)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    # Filtrer les paragraphes trop courts (titres seuls, etc.)
    valid_paragraphs = []
    for para in paragraphs:
        if len(para) >= 30:  # Minimum 30 chars pour Ã©viter titres isolÃ©s
            valid_paragraphs.append(para)
    
    return valid_paragraphs

def build_index():
    """Construit l'index de recherche avec dÃ©coupage intelligent par paragraphes."""
    chunks = []

    for txt_file in DATA_DIR.glob("*.txt"):
        raw_text = txt_file.read_text(encoding="utf-8")
        cleaned_text = clean_text(raw_text)
        
        paragraphs = split_into_paragraphs(cleaned_text)
        
        for paragraph in paragraphs:
            chunks.append({
                "source": txt_file.name,
                "content": paragraph
            })

    # Sauvegarder dans chunks.json
    INDEX_FILE.write_text(json.dumps(chunks, ensure_ascii=False, indent=2), encoding="utf-8")
    
    print(f"âœ… Index crÃ©Ã© avec succÃ¨s ({len(chunks)} chunks)")
    print(f"ğŸ“„ Fichiers traitÃ©s : {len(list(DATA_DIR.glob('*.txt')))}")
    print(f"ğŸ’¾ SauvegardÃ© dans : {INDEX_FILE}")

if __name__ == "__main__":
    build_index()