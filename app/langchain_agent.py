"""
Agent LangChain simplifiÃ© pour l'IMT utilisant Gemini.

Version compatible LangChain 1.x - Architecture simple sans ReAct.
"""
import os
import logging
from typing import Optional
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

from app.tools import search_imt, send_email

# Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()

# Prompt systÃ¨me
SYSTEM_PROMPT = """Tu es un assistant IA pour l'IMT (Institut Mines-TÃ©lÃ©com) au SÃ©nÃ©gal.

Tu peux :
1. Rechercher des informations sur l'IMT (formations, admissions, contact)
2. Envoyer des emails de contact

Directives :
- RÃ©ponds TOUJOURS en franÃ§ais
- Sois poli, professionnel et serviable
- Si tu n'es pas sÃ»r, cherche l'information
- Donne des rÃ©ponses prÃ©cises et complÃ¨tes
"""


def create_imt_agent(temperature: float = 0.3, verbose: bool = False):
    """CrÃ©e un agent LangChain simple.
    
    Args:
        temperature: TempÃ©rature pour la gÃ©nÃ©ration
        verbose: Mode verbeux
        
    Returns:
        Instance ChatGoogleGenerativeAI configurÃ©e
        
    Raises:
        ValueError: Si GEMINI_API_KEY manquante
    """
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY ou GOOGLE_API_KEY manquante dans .env")
    
    logger.info("âœ… Initialisation agent LangChain avec Gemini")
    
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        temperature=temperature,
        google_api_key=api_key
    )
    
    return llm


def run_agent(question: str, agent: Optional[ChatGoogleGenerativeAI] = None) -> str:
    """ExÃ©cute l'agent avec une question.
    
    Logique simple :
    1. DÃ©tecte si besoin de recherche IMT
    2. Appelle search_imt si nÃ©cessaire
    3. GÃ©nÃ¨re rÃ©ponse avec contexte
    
    Args:
        question: Question utilisateur
        agent: Agent LLM (crÃ©Ã© si None)
        
    Returns:
        RÃ©ponse gÃ©nÃ©rÃ©e
    """
    if not question or not question.strip():
        return "Veuillez poser une question valide."
    
    # CrÃ©er agent si nÃ©cessaire
    if agent is None:
        try:
            agent = create_imt_agent()
        except ValueError as e:
            logger.error(f"Erreur crÃ©ation agent: {e}")
            return "Agent non disponible (clÃ© API manquante)."
    
    try:
        # DÃ©tecter besoin de recherche
        keywords_search = ['formation', 'admission', 'contact', 'programme', 
                          'cybersÃ©curitÃ©', 'master', 'bachelor', 'imt', 'Ã©cole']
        needs_search = any(kw in question.lower() for kw in keywords_search)
        
        context = ""
        if needs_search:
            logger.info("ğŸ” Recherche IMT activÃ©e")
            search_results = search_imt(question)
            if search_results:
                context = f"\n\nContexte trouvÃ© :\n{search_results}\n"
        
        # Construire messages
        messages = [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=f"{question}{context}")
        ]
        
        # Appeler LLM
        logger.info(f"ğŸ¤– Appel Gemini via LangChain")
        response = agent.invoke(messages)
        
        result = response.content.strip()
        logger.info(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e ({len(result)} caractÃ¨res)")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur agent: {e}")
        return f"DÃ©solÃ©, une erreur s'est produite : {str(e)}"


# Fonction wrapper pour compatibilitÃ©
def create_and_run(question: str) -> str:
    """CrÃ©e un agent et exÃ©cute une question (usage simple)."""
    return run_agent(question)


if __name__ == "__main__":
    # Test rapide
    print("ğŸ§ª Test agent LangChain simplifiÃ©")
    test_question = "Quelles sont les formations proposÃ©es Ã  l'IMT ?"
    response = create_and_run(test_question)
    print(f"\nQuestion: {test_question}")
    print(f"RÃ©ponse: {response}")