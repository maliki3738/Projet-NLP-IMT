# app/agent.py

import os
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Optional, List, Dict, Any

# Ensure project root is on sys.path when launched via Chainlit
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from app.tools import search_imt, send_email

# Tentative d'import Langfuse (observabilit√©)
LANGFUSE_AVAILABLE = False
langfuse_client = None

if os.getenv("LANGFUSE_PUBLIC_KEY") and os.getenv("LANGFUSE_SECRET_KEY"):
    try:
        from langfuse import Langfuse
        langfuse_client = Langfuse(
            public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
            secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
            host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
        )
        LANGFUSE_AVAILABLE = True
        logger = logging.getLogger(__name__)
        logger.info("‚úÖ Langfuse configur√© avec succ√®s")
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.warning(f"‚ö†Ô∏è Langfuse non disponible : {e}")
else:
    logger = logging.getLogger(__name__)
    logger.debug("Langfuse d√©sactiv√© (pas de cl√©s configur√©es)")

load_dotenv()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# -------------------------
# Agent minimal et comment√©
# -------------------------
# Principe :
# 1) Tenter d'utiliser le SDK officiel `google.generativeai` (Gemini) si install√©.
# 2) Si le SDK n'est pas pr√©sent ou si l'appel √©choue, utiliser une heuristique simple
#    pour d√©cider entre deux actions : `SEARCH` (r√©pondre) ou `EMAIL` (envoyer un e-mail).
# 3) Les outils `search_imt` et `send_email` restent inchang√©s et sont appel√©s selon la d√©cision.

# Tentative d'import du SDK Gemini (optionnelle)
GENAI_AVAILABLE = False
API_KEY = None
import chainlit as cl

@cl.on_chat_start
async def _minimum_start():
    await cl.Message(
        content="‚úÖ Agent d√©marr√© correctement"
    ).send()

try:
    import requests
    API_KEY = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if API_KEY:
        # Test rapide de la cl√©
        test_url = f"https://generativelanguage.googleapis.com/v1/models?key={API_KEY}"
        response = requests.get(test_url, timeout=5)
        if response.status_code == 200:
            GENAI_AVAILABLE = True
            logger.info("‚úÖ Gemini API REST configur√© avec succ√®s")
        else:
            logger.warning(f"‚ö†Ô∏è Cl√© API Gemini invalide (status {response.status_code})")
    else:
        logger.warning("‚ö†Ô∏è Cl√© API Gemini manquante - Fallback heuristique activ√©")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Gemini non disponible : {e} - Fallback heuristique activ√©")

# Tentative d'import Grok/xAI comme alternative
GROK_AVAILABLE = False
grok_client = None
try:
    import openai
    GROK_API_KEY = os.getenv("XAI_API_KEY") or os.getenv("GROK_API_KEY")
    if GROK_API_KEY:
        grok_client = openai.OpenAI(
            api_key=GROK_API_KEY,
            base_url="https://api.x.ai/v1"
        )
        GROK_AVAILABLE = True
        logger.info("‚úÖ Grok (xAI) configur√© avec succ√®s")
except Exception as e:
    logger.info(f"üí° Grok non disponible : {e}")

# Configuration OpenAI GPT (fallback √©conomique)
OPENAI_AVAILABLE = False
openai_client = None
try:
    import openai
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    if OPENAI_API_KEY:
        openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        OPENAI_AVAILABLE = True
        logger.info("‚úÖ OpenAI GPT configur√© avec succ√®s")
except Exception as e:
    logger.info(f"üí° OpenAI non disponible : {e}")

# Flag global pour tracker si tous les LLMs ont √©chou√© (√©viter de les rappeler)
_all_llms_failed = False

def _call_grok(prompt: str, max_tokens: int = 150) -> Optional[str]:
    """Appelle Grok via l'API xAI avec tra√ßabilit√© Langfuse.
    
    Args:
        prompt: Le prompt √† envoyer
        max_tokens: Nombre max de tokens
    
    Returns:
        La r√©ponse ou None
    """
    if not GROK_AVAILABLE or not grok_client:
        return None
    
    try:
        response = grok_client.chat.completions.create(
            model="grok-beta",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.3,
        )
        result = response.choices[0].message.content.strip()
        
        # Trace Langfuse 3.x avec usage tokens + co√ªts Grok
        if LANGFUSE_AVAILABLE:
            try:
                # Grok pricing : $5/1M input tokens, $15/1M output tokens
                usage = response.usage
                input_tokens = usage.prompt_tokens if usage else 0
                output_tokens = usage.completion_tokens if usage else 0
                cost = (input_tokens / 1_000_000 * 5.0) + (output_tokens / 1_000_000 * 15.0)
                
                langfuse_client.create_event(
                    name="grok_call",
                    metadata={
                        "model": "grok-beta",
                        "max_tokens": max_tokens,
                        "cost_usd": round(cost, 6)
                    },
                    input={"prompt": prompt},
                    output={"response": result},
                    usage={
                        "prompt_tokens": input_tokens,
                        "completion_tokens": output_tokens,
                        "total_tokens": input_tokens + output_tokens
                    }
                )
            except Exception as trace_error:
                logger.warning(f"Langfuse trace failed: {trace_error}")
        
        return result
    except Exception as e:
        logger.error(f"Erreur Grok : {e}")
        # Trace error in Langfuse
        if LANGFUSE_AVAILABLE:
            try:
                langfuse_client.create_event(
                    name="grok_call_error",
                    metadata={"model": "grok-beta", "error": str(e)},
                    input={"prompt": prompt}
                )
            except:
                pass
        return None

def _call_openai(prompt: str, max_tokens: int = 200) -> Optional[str]:
    """Appelle OpenAI GPT-4o-mini (√©conomique et performant) avec tra√ßabilit√© Langfuse.
    
    Args:
        prompt: Le prompt √† envoyer
        max_tokens: Nombre max de tokens
    
    Returns:
        La r√©ponse ou None
    """
    if not OPENAI_AVAILABLE or not openai_client:
        return None
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",  # Le moins cher : 0.15$/1M tokens entr√©e, 0.6$/1M sortie
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.3,
        )
        result = response.choices[0].message.content.strip()
        
        # Trace Langfuse 3.x avec usage tokens + co√ªts OpenAI
        if LANGFUSE_AVAILABLE:
            try:
                # OpenAI GPT-4o-mini pricing : $0.15/1M input, $0.60/1M output
                usage = response.usage
                input_tokens = usage.prompt_tokens if usage else 0
                output_tokens = usage.completion_tokens if usage else 0
                cost = (input_tokens / 1_000_000 * 0.15) + (output_tokens / 1_000_000 * 0.60)
                
                langfuse_client.create_event(
                    name="openai_call",
                    metadata={
                        "model": "gpt-4o-mini",
                        "max_tokens": max_tokens,
                        "cost_usd": round(cost, 6)
                    },
                    input={"prompt": prompt},
                    output={"response": result},
                    usage={
                        "prompt_tokens": input_tokens,
                        "completion_tokens": output_tokens,
                        "total_tokens": input_tokens + output_tokens
                    }
                )
            except Exception as trace_error:
                logger.warning(f"Langfuse trace failed: {trace_error}")
        
        return result
    except Exception as e:
        logger.error(f"Erreur OpenAI : {e}")
        # Trace error in Langfuse
        if LANGFUSE_AVAILABLE:
            try:
                langfuse_client.create_event(
                    name="openai_call_error",
                    metadata={"model": "gpt-4o-mini", "error": str(e)},
                    input={"prompt": prompt}
                )
            except:
                pass
        return None

def _call_gemini(prompt: str) -> Optional[str]:
    """Appelle les LLMs disponibles avec ordre de priorit√© intelligent.
    
    ‚ú® NOUVEL ORDRE : Gemini (gratuit) ‚Üí Grok ‚Üí OpenAI ‚Üí None
    
    Gemini en priorit√© car :
    - Free tier : 15 req/min, 1500 req/jour
    - Gratuit (0$) avec tracking tokens dans Langfuse
    - Mod√®le performant : gemini-2.0-flash-exp
    
    Instructions pour l'IA:
    Tu es l'expert de l'IMT Dakar. Utilise les documents fournis pour r√©pondre. 
    Si l'information est absente, ne l'invente pas, oriente vers l'administration. 
    R√©ponds en faisant des phrases compl√®tes et polies.

    Retourne la cha√Æne textuelle de la r√©ponse, ou `None` en cas d'erreur.
    """
    global _all_llms_failed
    
    # ‚≠ê PRIORIT√â 1 : Essayer Gemini (GRATUIT)
    if GENAI_AVAILABLE:
        logger.debug("ü•á Tentative Gemini (priorit√© 1)...")
        result = _call_gemini_direct(prompt)
        if result:
            logger.info("‚úÖ Gemini a r√©pondu")
            return result
        logger.info("üîÑ Gemini √©chou√©, fallback vers Grok...")
    
    # Priorit√© 2 : Essayer Grok
    if GROK_AVAILABLE:
        logger.debug("ü•à Tentative Grok (priorit√© 2)...")
        result = _call_grok(prompt, max_tokens=150)
        if result:
            logger.info("‚úÖ Grok a r√©pondu")
            return result
        logger.info("üîÑ Grok √©chou√©, fallback vers OpenAI...")
    
    # Priorit√© 3 : Essayer OpenAI (√©conomique mais payant)
    if OPENAI_AVAILABLE:
        logger.debug("ü•â Tentative OpenAI (priorit√© 3)...")
        result = _call_openai(prompt, max_tokens=200)
        if result:
            logger.info("‚úÖ OpenAI a r√©pondu")
            return result
        logger.info("‚ùå OpenAI √©chou√©, aucun LLM disponible")
    
    # Tous les LLM ont √©chou√© - setter le flag pour √©viter de les rappeler
    logger.debug("Tous les LLM ont √©chou√©, retour None")
    _all_llms_failed = True
    return None

def _call_gemini_direct(prompt: str) -> Optional[str]:
    """Appel direct √† Gemini via API REST (plus stable que le SDK).
    
    Returns:
        La r√©ponse de Gemini ou None si erreur.
    """
    if not GENAI_AVAILABLE or not API_KEY:
        return None
    
    try:
        import requests
        import json
        
        url = f"https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash:generateContent?key={API_KEY}"
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 300,
            }
        }
        
        logger.debug(f"Appel Gemini API REST avec prompt: {prompt[:50]}...")
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code != 200:
            logger.error(f"Gemini API error {response.status_code}: {response.text[:200]}")
            return None
        
        data = response.json()
        
        # Extraction du texte de la r√©ponse
        if 'candidates' not in data or not data['candidates']:
            logger.warning("R√©ponse Gemini vide ou malform√©e")
            return None
        
        result = data['candidates'][0]['content']['parts'][0]['text'].strip()
        logger.debug(f"R√©ponse Gemini: {result[:100]}...")
        
        # Track dans Langfuse avec usage tokens
        if LANGFUSE_AVAILABLE:
            try:
                usage = data.get('usageMetadata', {})
                usage_dict = {
                    "prompt_tokens": usage.get('promptTokenCount', 0),
                    "completion_tokens": usage.get('candidatesTokenCount', 0),
                    "total_tokens": usage.get('totalTokenCount', 0)
                }
                
                langfuse_client.create_event(
                    name="gemini_call",
                    metadata={
                        "model": "gemini-2.0-flash",
                        "temperature": 0.3,
                        "max_tokens": 300,
                        "cost": 0.0  # Free tier
                    },
                    input={"prompt": prompt},
                    output={"response": result},
                    usage=usage_dict
                )
            except Exception as trace_error:
                logger.warning(f"Langfuse trace failed: {trace_error}")
        
        return result
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Erreur lors de l'appel Gemini : {error_msg[:200]}")
        if LANGFUSE_AVAILABLE:
            try:
                langfuse_client.create_event(
                    name="gemini_call_error",
                    metadata={"model": "gemini-2.0-flash", "error": error_msg[:500]},
                    input={"prompt": prompt[:200]}
                )
            except:
                pass
        return None


def _extract_personal_info(question: str) -> dict:
    """Extrait les informations personnelles de la question.
    
    Returns:
        dict: {entity_type: value} ex: {'name': 'Maliki'}
    """
    import re
    entities = {}
    q_lower = question.lower().strip()
    
    # Ignorer les questions (ne pas extraire de nom)
    question_words = ['comment', 'qui', 'quoi', 'quel', 'quelle', 'o√π', 'pourquoi', 'quand']
    if any(q_lower.startswith(word) for word in question_words):
        return entities
    
    # Pattern : "je m'appelle X" ou "mon nom est X"
    # Capturer le nom (premi√®re lettre majuscule ou pas, on normalisera apr√®s)
    name_patterns = [
        r"(?:je m['']appelle|retiens que je m['']appelle)\s+([A-Z√Ä-≈∏a-z√©√®√™√†√¢√Æ√¥√ª√ß]+(?:\s+[A-Z√Ä-≈∏a-z√©√®√™√†√¢√Æ√¥√ª√ß]+)?)",
        r"mon nom (?:est|c'est)\s+([A-Z√Ä-≈∏a-z√©√®√™√†√¢√Æ√¥√ª√ß]+(?:\s+[A-Z√Ä-≈∏a-z√©√®√™√†√¢√Æ√¥√ª√ß]+)?)",
    ]
    
    for pattern in name_patterns:
        match = re.search(pattern, question, re.IGNORECASE)
        if match:
            name = match.group(1).strip()
            # V√©rifier que ce n'est pas un mot de question
            if name.lower() not in question_words:
                # Capitaliser le nom proprement
                entities['name'] = ' '.join(word.capitalize() for word in name.split())
                break
    # Pattern : "je suis un/une X" (genre, profil, etc.)
    profile_pattern = r"je suis (?:un|une)\s+(.+?)(?:\.|$|,)"
    profile_match = re.search(profile_pattern, q_lower)
    if profile_match:
        entities['profile'] = profile_match.group(1).strip()
    
    # Pattern email : "mon email est X" ou "mon adresse est X"
    email_pattern = r"mon (?:email|e-mail|adresse|mail) (?:est|c'est)\s+([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})"
    email_match = re.search(email_pattern, q_lower)
    if email_match:
        entities['email'] = email_match.group(1)
    
    # Pattern t√©l√©phone : "mon t√©l√©phone/num√©ro est X"
    phone_pattern = r"mon (?:t√©l√©phone|num√©ro|tel) (?:est|c'est)\s+([+]?[0-9\s]+)"
    phone_match = re.search(phone_pattern, q_lower)
    if phone_match:
        entities['phone'] = phone_match.group(1).strip()
    
    return entities


def _answer_personal_question(question: str, entities: dict) -> str:
    """R√©pond aux questions personnelles en utilisant les entit√©s stock√©es."""
    q_lower = question.lower().strip()
    
    # Questions sur le nom
    if any(phrase in q_lower for phrase in ["je m'appelle", "mon nom", "comment je", "qui suis-je", "appelle comment"]):
        if 'name' in entities:
            return f"üë§ Vous vous appelez **{entities['name']}**."
        else:
            return "Je ne connais pas encore votre nom. Vous pouvez me le dire en disant 'Je m'appelle [votre nom]'."
    
    # Questions sur le profil
    if any(phrase in q_lower for phrase in ["qui suis-je", "je suis qui", "mon profil", "c'est quoi mon profil"]):
        if 'profile' in entities:
            response = f"üë§ Vous √™tes **{entities['profile']}**."
            if 'name' in entities:
                response = f"üë§ Vous vous appelez **{entities['name']}** et vous √™tes **{entities['profile']}**."
            return response
    
    # Questions sur l'email
    if any(word in q_lower for word in ["email", "e-mail", "adresse mail"]):
        if 'email' in entities:
            return f"üìß Votre email est **{entities['email']}**."
        else:
            return "Je ne connais pas votre email."
    
    # Questions sur le t√©l√©phone
    if any(word in q_lower for word in ["t√©l√©phone", "num√©ro", "tel"]):
        if 'phone' in entities:
            return f"üìû Votre num√©ro est **{entities['phone']}**."
        else:
            return "Je ne connais pas votre num√©ro de t√©l√©phone."
    
    return None

def agent(question: str, history: list = None, memory_manager=None, session_id: str = None) -> str:
    """Fonction principale de l'agent.

    - Construit un prompt simple demandant `SEARCH` ou `EMAIL`.
    - Tente d'obtenir la d√©cision via Gemini (_call_gemini).
    - Si √©chec, applique une heuristique de mots-cl√©s.
    - Ex√©cute ensuite l'outil appropri√© et retourne son r√©sultat.
    """
    if not question or not question.strip():
        logger.warning("Question vide re√ßue")
        return "D√©sol√©, je n'ai pas compris votre question. Pouvez-vous reformuler ?"
    
    # 1. Extraire et stocker les informations personnelles
    if memory_manager and session_id:
        personal_info = _extract_personal_info(question)
        if personal_info:
            for entity_type, value in personal_info.items():
                memory_manager.set_entity(session_id, entity_type, value)
                logger.info(f"‚úÖ Entit√© stock√©e : {entity_type} = {value}")
            
            # R√©pondre √† la confirmation
            if 'name' in personal_info:
                return f"üëã Enchant√© **{personal_info['name']}** ! Je vais me souvenir de votre nom."
            elif 'profile' in personal_info:
                return f"‚úÖ J'ai bien not√© : vous √™tes **{personal_info['profile']}**."
            elif 'email' in personal_info:
                return f"‚úÖ J'ai bien not√© votre email : **{personal_info['email']}**"
            elif 'phone' in personal_info:
                return f"‚úÖ J'ai bien not√© votre num√©ro : **{personal_info['phone']}**"
    
    # 2. V√©rifier si c'est une question personnelle
    if memory_manager and session_id:
        entities = memory_manager.get_all_entities(session_id)
        personal_answer = _answer_personal_question(question, entities)
        if personal_answer:
            return personal_answer
    
    logger.info(f"Question re√ßue : {question}")
    
    # 3. Enrichir les questions courtes avec le contexte de la conversation
    enriched_question = question
    if memory_manager and session_id and len(question.split()) <= 3:
        recent_history = _get_recent_history(memory_manager, session_id, limit=2)
        last_context = _extract_user_context(recent_history)
        if last_context:
            enriched_question = f"{last_context}. {question}"
            logger.info(f"Question enrichie: {enriched_question}")
    
    try:
        prompt = (
            "Tu es un agent pour l'IMT. R√©ponds UNIQUEMENT par SEARCH ou EMAIL.\n"
            f"Question : {enriched_question}\n"
        )

        decision = _call_gemini(prompt)

        # Si LLM absent ou probl√®me, heuristique simple bas√©e sur mots-cl√©s
        if not decision:
            logger.info("Utilisation du fallback heuristique")
            q = enriched_question.lower()
            # Mots-cl√©s enrichis pour EMAIL
            email_keywords = [
                "directeur", "email", "envoyer", "envoye", "envoi", 
                "contact", "contacter", "√©crire", "message", "demande officielle"
            ]
            if any(k in q for k in email_keywords):
                decision = "EMAIL"
            else:
                decision = "SEARCH"

        decision = decision.strip().upper()
        logger.info(f"D√©cision prise : {decision}")

        if "EMAIL" in decision:
            # Appel de l'outil d'envoi d'email
            logger.info("Ex√©cution : Envoi d'email")
            
            # Extraire le sujet et le message avec les nouvelles regex am√©lior√©es
            import re
            subject_match = re.search(r'(?:avec|pour)\s+(?:comme\s+)?(?:pour\s+)?objet\s+["\']([^"\']+)["\']', question, re.IGNORECASE)
            subject = subject_match.group(1) if subject_match else "Demande d'informations"
            
            # Extraire le vrai message si format "envoie un mail avec message X"
            message_match = re.search(r'(?:disant que|avec (?:comme )?message|message)\s+["\']?([^"\']+?)["\']\s*(?:avec|pour|$)', question, re.IGNORECASE)
            if not message_match:
                message_match = re.search(r'(?:disant que|message[\s:]+)(.+?)(?:\s+avec|\s+pour|$)', question, re.IGNORECASE)
            
            content = message_match.group(1).strip() if message_match else question
            
            # Nettoyer le contenu (enlever "avec pour objet..." s'il y est)
            content = re.sub(r'\s*avec\s+pour\s+objet.+$', '', content, flags=re.IGNORECASE)
            
            return send_email(subject=subject, content=content)
        
        # Par d√©faut, on appelle la recherche
        logger.info("Ex√©cution : Recherche IMT")
        raw_context = search_imt(enriched_question)
        return reformulate_answer(enriched_question, raw_context)
    
    except Exception as e:
        logger.error(f"Erreur critique dans l'agent : {e}", exc_info=True)
        return "D√©sol√©, une erreur s'est produite. Veuillez r√©essayer ou reformuler votre question."

def _deduplicate_lines(text: str) -> str:
    """Supprime les lignes dupliqu√©es cons√©cutives."""  
    lines = text.split('\n')
    result = []
    prev = None
    for line in lines:
        stripped = line.strip()
        if stripped != prev:
            result.append(line)
            prev = stripped
    return '\n'.join(result)

def _get_recent_history(memory_manager, session_id: str, limit: int = 2) -> List[Any]:
    """Safely fetch recent history with compatibility across memory backends."""
    try:
        return memory_manager.get_history(session_id, limit=limit)
    except TypeError:
        history = memory_manager.get_history(session_id)
        return history[-limit:] if history else []
    except Exception:
        return []

def _extract_user_context(history: List[Any]) -> str:
    """Extract the last user messages from a history list (dicts or 'role: msg' strings)."""
    if not history:
        return ""
    user_msgs: List[str] = []
    for msg in history[-2:]:
        if isinstance(msg, dict):
            if msg.get("role") == "user":
                user_msgs.append((msg.get("content") or "").strip())
        elif isinstance(msg, str):
            if msg.lower().startswith("user:"):
                user_msgs.append(msg.split(":", 1)[1].strip())
    return " ".join([m for m in user_msgs if m])

def reformulate_answer(question: str, context: str) -> str:
    """Reformule la r√©ponse en utilisant Grok/Gemini avec des instructions claires."""
    global _all_llms_failed
    
    if not context or context.strip() == "":
        logger.warning("Contexte vide pour reformulation")
        return "D√©sol√©, je n'ai pas trouv√© d'information pertinente sur cette question."
    
    # Nettoyer le contexte
    context = _deduplicate_lines(context.strip())
    context = context.replace('===', '').replace('[', '').replace(']', '')
    
    # Ne PAS essayer les LLMs s'ils ont tous √©chou√© (√©vite le segfault avec Gemini SDK)
    if not _all_llms_failed and (GENAI_AVAILABLE or GROK_AVAILABLE or OPENAI_AVAILABLE):
        try:
            prompt = f"""Tu es un assistant expert de l'Institut Mines-T√©l√©com (IMT) √† Dakar.

CONTEXTE DOCUMENTAIRE :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS IMPORTANTES :
- R√©ponds en fran√ßais de mani√®re claire, concise et professionnelle
- Utilise UNIQUEMENT les informations pr√©sentes dans le contexte documentaire
- Si l'information est absente, ne l'invente pas - oriente vers l'administration
- Ne mentionne PAS que tu utilises un contexte ou des documents
- Sois naturel, direct et accueillant comme un conseiller d'√©tudes
- R√©ponds en phrases compl√®tes et polies

R√âPONSE :"""
            
            llm_response = _call_gemini(prompt)
            if llm_response:
                return llm_response
        except Exception as e:
            logger.debug(f"LLM reformulation failed: {e}")
    
    # Fallback intelligent : extraire le meilleur paragraphe du contexte
    logger.info("üí° Utilisation du fallback intelligent (extraction directe)")
    lines = [l.strip() for l in context.split('\n') if l.strip() and len(l.strip()) > 40]
    if lines:
        # Retourner les 3 premi√®res lignes pertinentes avec formatage
        result = '\n\n'.join(lines[:3])
        return f"üìö D'apr√®s nos documents :\n\n{result}\n\nüí° Pour plus d'informations, contactez l'administration de l'IMT Dakar."
    return f"üìö Voici ce que j'ai trouv√© :\n\n{context[:500]}\n\nüí° Pour plus d'informations, contactez l'administration."

try:
    import chainlit as cl
    import uuid
    from memory.redis_memory import RedisMemory
    from app.mysql_data_layer import MySQLDataLayer

    _memory = RedisMemory()

    # ‚ùå D√©sactiver MySQL temporairement (r√©activer plus tard si besoin)
    @cl.data_layer
    def get_data_layer():
        return MySQLDataLayer.from_env()

    @cl.on_chat_start
    async def _on_chat_start():
        session_id = str(uuid.uuid4())
        _memory.create_session(session_id)
        cl.user_session.set("session_id", session_id)
        await cl.Message(
            content="Bonjour ! Je suis l'assistant de l'Institut Mines-Telecom Dakar. Comment puis-je vous aider ?"
        ).send()

    @cl.on_message
    async def _on_message(message: cl.Message):
        user_message = message.content.strip()
        session_id = cl.user_session.get("session_id")
        if not session_id:
            session_id = str(uuid.uuid4())
            _memory.create_session(session_id)
            cl.user_session.set("session_id", session_id)

        _memory.add_message(session_id, "user", user_message)
        response = agent(user_message, memory_manager=_memory, session_id=session_id)
        _memory.add_message(session_id, "assistant", response)
        await cl.Message(content=response).send()

except Exception as e:
    logger.info(f"Chainlit non disponible ou erreur d'initialisation : {e}")

if __name__ == "__main__":
    print("Agent IMT pr√™t\n")
    while True:
        question = input("Posez votre question √† l'agent IMT (ou 'quit' pour quitter) : ")
        if question.lower() == 'quit':
            break
        response = agent(question)
        print(response)
        print("\n---\n")
